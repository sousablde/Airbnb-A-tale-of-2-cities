{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different conditions for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from math import sqrt\n",
    "import tests as t\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import folium\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import xgboost as xgb\n",
    "pd.set_option('display.max_columns', 106)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto = pd.read_csv(r\"C:\\Users\\sousa\\Desktop\\github\\Airbnb Tale of 2 cities\\Data Portugal\\porto_listings.csv\")\n",
    "lisbon = pd.read_csv(r\"C:\\Users\\sousa\\Desktop\\github\\Airbnb Tale of 2 cities\\Data Portugal\\lisbon_listings.csv\")\n",
    "\n",
    "porto_min = pd.read_csv(r\"C:\\Users\\sousa\\Desktop\\github\\Airbnb Tale of 2 cities\\Data Portugal\\porto_listings_min.csv\")\n",
    "lisbon_min = pd.read_csv(r\"C:\\Users\\sousa\\Desktop\\github\\Airbnb Tale of 2 cities\\Data Portugal\\lisbon_listings_min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisbon.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction, determining min components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scale and normalize the dataframes features\n",
    "def feature_scaling(df, type_scale):\n",
    "    '''\n",
    "    This function takes in either the azdias or the customers dataframe and applyes the selected feature scaler\n",
    "    Args: customer or azdias dataframe and a string representing the type of scaling intended\n",
    "    returns: scaled dataframe\n",
    "    '''\n",
    "    \n",
    "    features_list = df.columns\n",
    "    \n",
    "    if type_scale == 'StandardScaler':\n",
    "        df_scaled = StandardScaler().fit_transform(df)\n",
    "        \n",
    "    if type_scale == 'MinMaxScaler':\n",
    "        df_scaled = MinMaxScaler().fit_transform(df)\n",
    "    \n",
    "    df_scaled = pd.DataFrame(df_scaled)\n",
    "    df_scaled.columns = features_list\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca model\n",
    "def pca_model(df, n_components):\n",
    "    '''\n",
    "    This function defines a model that takes in a previously scaled dataframe and returns the result of \n",
    "    the transformation. The output is an onject created post data fitting\n",
    "    '''\n",
    "    pca = PCA(n_components)\n",
    "    pca_df = pca.fit(df)\n",
    "    \n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scree plots for PCA\n",
    "def scree_plots(SS1,SS2, dataname1, dataname2):\n",
    "    '''\n",
    "    This function takes in the transformed data using PCA and plots it in scree plots\n",
    "    '''\n",
    "    subplot(2,1,1)\n",
    "\n",
    "    plt.plot(np.cumsum(SS1.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance Ratio vs Number of Components SS' + dataname1)\n",
    "    plt.grid(b=True)\n",
    "\n",
    "    subplot(2,1,2)\n",
    "    plt.plot(np.cumsum(SS2.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance Ratio vs Number of Components SS' + dataname2)\n",
    "    plt.grid(b=True)\n",
    "\n",
    "    plot = tight_layout()\n",
    "    plot = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_SS = feature_scaling(porto, 'StandardScaler')\n",
    "lisbon_SS = feature_scaling(lisbon, 'StandardScaler')\n",
    "\n",
    "n_components_porto = len(porto_SS.columns.values)\n",
    "n_components_lisbon = len(lisbon_SS.columns.values)\n",
    "\n",
    "porto_SS_pca = pca_model(porto_SS, n_components_porto)\n",
    "lisbon_SS_pca = pca_model(lisbon_SS, n_components_lisbon)\n",
    "\n",
    "scree_plots(porto_SS_pca, lisbon_SS_pca, ' Porto', ' Lisbon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using standard scaler with 150 principal components 90% of the original variance can be represented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_comparisons(df, city):\n",
    "    \n",
    "    #getting target\n",
    "    X = df.drop('price', 1)\n",
    "    y = df['price']\n",
    "    \n",
    "    #split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    #no scaling and no dimensionality reduction\n",
    "    no_scale_no_pca = Pipeline([('linear', linear_model.LinearRegression())])\n",
    "    no_scale_no_pca.fit(X_train, y_train)\n",
    "    y_pred1 = no_scale_no_pca.predict(X_test)\n",
    "    linear_reg_err1 = metrics.median_absolute_error(y_test, y_pred1)\n",
    "    linear_reg_r21 = metrics.r2_score(y_test, y_pred1)\n",
    "\n",
    "\n",
    "    #with scaling, no dimensionality reduction\n",
    "    scale_no_pca = Pipeline([('standardize', StandardScaler()), ('linear', linear_model.LinearRegression())])\n",
    "    scale_no_pca.fit(X_train, y_train)\n",
    "    y_pred2 = scale_no_pca.predict(X_test)\n",
    "    linear_reg_err2 = metrics.median_absolute_error(y_test, y_pred2)\n",
    "    linear_reg_r22 = metrics.r2_score(y_test, y_pred2)\n",
    "\n",
    "\n",
    "    #scaling and dimensionality reduction\n",
    "    scale_pca = Pipeline([('standardize', StandardScaler()),\n",
    "        ('pca', PCA(n_components = 125)), ('linear', linear_model.LinearRegression())])\n",
    "    scale_pca.fit(X_train, y_train)\n",
    "    y_pred3 = scale_pca.predict(X_test)\n",
    "    linear_reg_err3 = metrics.median_absolute_error(y_test, y_pred3)\n",
    "    linear_reg_r23 = metrics.r2_score(y_test, y_pred3)\n",
    "\n",
    "\n",
    "    print (\"Linear Regression's MAE without PCA or scaling in \" + city + \" : \" + str(linear_reg_err1))\n",
    "    print (\"Linear Regression's MAE deviation with scaling in \" + city + \" : \" + str(linear_reg_err2))\n",
    "    print (\"Linear Regression's MAE deviation with PCA and scaling in \" + city + \" : \" + str(linear_reg_err3))\n",
    "\n",
    "    print (\"Linear Regression's r2 without PCA or scaling in \" + city + \" : \"+ str(linear_reg_r21))\n",
    "    print (\"Linear Regression's r2 deviation with scaling in \" + city + \" : \"+ str(linear_reg_r22))\n",
    "    print (\"Linear Regression's r2 deviation with PCA and scaling in \" + city + \" : \" + str(linear_reg_r23))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_comparisons(porto, 'porto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_comparisons(lisbon, 'lisbon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_comparisons(porto_min, 'porto min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_comparisons(lisbon_min, 'lisbon min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_porto = porto.drop('price', 1)\n",
    "y_porto = porto['price']\n",
    "\n",
    "X_lisbon = lisbon.drop('price', 1)\n",
    "y_lisbon = lisbon['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data\n",
    "X_porto_train, X_porto_test, y_porto_train, y_porto_test = train_test_split(X_porto, y_porto, test_size=0.2)\n",
    "\n",
    "X_lisbon_train, X_lisbon_test, y_lisbon_train, y_lisbon_test = train_test_split(X_lisbon, y_lisbon, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with no scaling, scalling, with pca and without pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no scaling and no dimensionality reduction\n",
    "no_scale_no_pca = Pipeline([('linear', linear_model.LinearRegression())])\n",
    "no_scale_no_pca.fit(X_porto_train, y_porto_train)\n",
    "y_pred1_porto = no_scale_no_pca.predict(X_porto_test)\n",
    "linear_reg_err1_porto = metrics.median_absolute_error(y_porto_test, y_pred1_porto)\n",
    "linear_reg_r21_porto = metrics.r2_score(y_porto_test, y_pred1_porto)\n",
    "\n",
    "\n",
    "#with scaling, no dimensionality reduction\n",
    "scale_no_pca = Pipeline([('standardize', StandardScaler()), ('linear', linear_model.LinearRegression())])\n",
    "scale_no_pca.fit(X_porto_train, y_porto_train)\n",
    "y_pred2_porto = scale_no_pca.predict(X_porto_test)\n",
    "linear_reg_err2_porto = metrics.median_absolute_error(y_porto_test, y_pred2_porto)\n",
    "linear_reg_r22_porto = metrics.r2_score(y_porto_test, y_pred2_porto)\n",
    "\n",
    "    \n",
    "#scaling and dimensionality reduction\n",
    "scale_pca = Pipeline([('standardize', StandardScaler()),\n",
    "    ('pca', PCA(n_components = 125)), ('linear', linear_model.LinearRegression())])\n",
    "scale_pca.fit(X_porto_train, y_porto_train)\n",
    "y_pred3_porto = scale_pca.predict(X_porto_test)\n",
    "linear_reg_err3_porto = metrics.median_absolute_error(y_porto_test, y_pred3_porto)\n",
    "linear_reg_r23_porto = metrics.r2_score(y_porto_test, y_pred3_porto)\n",
    "\n",
    "\n",
    "print (\"Linear Regression's MAE without PCA or scaling in Porto: \" + str(linear_reg_err1_porto))\n",
    "print (\"Linear Regression's MAE deviation with scaling in Porto: \" + str(linear_reg_err2_porto))\n",
    "print (\"Linear Regression's MAE deviation with PCA and scaling in Porto: \" + str(linear_reg_err3_porto))\n",
    "\n",
    "print (\"Linear Regression's r2 without PCA or scaling in Porto: \" + str(linear_reg_r21_porto))\n",
    "print (\"Linear Regression's r2 deviation with scaling in Porto: \" + str(linear_reg_r22_porto))\n",
    "print (\"Linear Regression's r2 deviation with PCA and scaling in Porto: \" + str(linear_reg_r23_porto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_porto_test, y_pred1_porto, c = 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no scaling and no dimensionality regression\n",
    "no_scale_no_pca = Pipeline([('linear', linear_model.LinearRegression())])\n",
    "no_scale_no_pca.fit(X_lisbon_train, y_lisbon_train)\n",
    "y_pred1_lisbon = no_scale_no_pca.predict(X_lisbon_test)\n",
    "linear_reg_err1_lisbon = metrics.median_absolute_error(y_lisbon_test, y_pred1_lisbon)\n",
    "linear_reg_r21_lisbon = metrics.r2_score(y_lisbon_test, y_pred1_lisbon)\n",
    "\n",
    "\n",
    "#with scaling, no dimensionality regression\n",
    "scale_no_pca = Pipeline([('standardize', StandardScaler()), ('linear', linear_model.LinearRegression())])\n",
    "scale_no_pca.fit(X_lisbon_train, y_lisbon_train)\n",
    "y_pred2_lisbon = scale_no_pca.predict(X_lisbon_test)\n",
    "linear_reg_err2_lisbon = metrics.median_absolute_error(y_lisbon_test, y_pred2_lisbon)\n",
    "linear_reg_r22_lisbon = metrics.r2_score(y_lisbon_test, y_pred2_lisbon)\n",
    "\n",
    "    \n",
    "#scaling and dimensionality reduction\n",
    "scale_pca = Pipeline([('standardize', StandardScaler()),\n",
    "    ('pca', PCA(n_components = 125)), ('linear', linear_model.LinearRegression())])\n",
    "scale_pca.fit(X_lisbon_train, y_lisbon_train)\n",
    "y_pred3_lisbon = scale_pca.predict(X_lisbon_test)\n",
    "linear_reg_err3_lisbon = metrics.median_absolute_error(y_lisbon_test, y_pred3_lisbon)\n",
    "linear_reg_r23_lisbon = metrics.r2_score(y_lisbon_test, y_pred3_lisbon)\n",
    "\n",
    "\n",
    "print (\"Linear Regression's MAE without PCA or scaling in Lisbon: \" + str(linear_reg_err1_lisbon))\n",
    "print (\"Linear Regression's MAE deviation with scaling in Lisbon: \" + str(linear_reg_err2_lisbon))\n",
    "print (\"Linear Regression's MAE deviation with PCA and scaling in Lisbon: \" + str(linear_reg_err3_lisbon))\n",
    "print (\"Linear Regression's r2 without PCA or scaling in Lisbon: \" + str(linear_reg_r21_lisbon))\n",
    "print (\"Linear Regression's r2 deviation with scaling in Lisbon: \" + str(linear_reg_r22_lisbon))\n",
    "print (\"Linear Regression's r2 deviation with PCA and scaling in Lisbon: \" + str(linear_reg_r23_lisbon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpm_reg_start = time.time()\n",
    "\n",
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_reg = LinearRegression()  \n",
    "hpm_reg.fit(X_porto_train, y_porto_train) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_reg = hpm_reg.predict(X_porto_train)\n",
    "val_preds_hpm_reg = hpm_reg.predict(X_porto_test)\n",
    "\n",
    "hpm_reg_end = time.time()\n",
    "\n",
    "print(f\"Time taken to run: {round((hpm_reg_end - hpm_reg_start)/60,1)} minutes\")\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining RMSE:\", round(mean_squared_error(y_porto_train, training_preds_hpm_reg),4))\n",
    "print(\"Validation RMSE:\", round(mean_squared_error(y_porto_test, val_preds_hpm_reg),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_porto_train, training_preds_hpm_reg),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_porto_test, val_preds_hpm_reg),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_array = np.array(list(y_porto_test))\n",
    "val_preds_hpm_reg_array = np.array(val_preds_hpm_reg)\n",
    "hpm_df = pd.DataFrame({'Actual': y_test_array.flatten(), 'Predicted': val_preds_hpm_reg_array.flatten()})\n",
    "hpm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "for i in range (-2, 3):\n",
    "    alpha = 10**i\n",
    "    rm = linear_model.Ridge(alpha=alpha)\n",
    "    ridge_model = rm.fit(X_porto_train, y_porto_train)\n",
    "    preds_ridge = ridge_model.predict(X_porto_test)\n",
    "\n",
    "    plt.scatter(preds_ridge, y_porto_test, alpha=.75, color='r')\n",
    "    plt.xlabel('Predicted Price')\n",
    "    plt.ylabel('Actual Price')\n",
    "    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n",
    "    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n",
    "                   round(ridge_model.score(X_porto_test, y_porto_test), 4),\n",
    "                    round(mean_squared_error(y_porto_train, training_preds_hpm_reg),4))\n",
    "    plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost Regressor no scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = xgb.XGBRegressor()\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300],\n",
    "              'learning_rate': [0.01, 0.05, 0.1], \n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.6, 0.7, 1],\n",
    "              'gamma': [0.0, 0.1, 0.2]}\n",
    "\n",
    "booster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tuned random forest porto\n",
    "booster_grid_search.fit(X_porto_train, y_porto_train)\n",
    "\n",
    "print(booster_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tuned random forest lisbon\n",
    "booster_grid_search.fit(X_lisbon_train, y_lisbon_train)\n",
    "\n",
    "print(booster_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the models based on the hyperparameters found by grid search\n",
    "booster_porto = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, \n",
    "                           max_depth=3, n_estimators=300, random_state=4)\n",
    "\n",
    "booster_lisbon = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n",
    "                           max_depth=6, n_estimators=300, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "booster_porto.fit(X_porto_train, y_porto_train)\n",
    "\n",
    "booster_lisbon.fit(X_lisbon_train, y_lisbon_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred_train_porto = booster_porto.predict(X_porto_train)\n",
    "y_pred_test_porto = booster_porto.predict(X_porto_test)\n",
    "\n",
    "y_pred_train_lisbon = booster_lisbon.predict(X_lisbon_train)\n",
    "y_pred_test_lisbon = booster_lisbon.predict(X_lisbon_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics to determine quality of model\n",
    "RMSE_porto = np.sqrt(mean_squared_error(y_porto_test, y_pred_test_porto))\n",
    "RMSE_lisbon = np.sqrt(mean_squared_error(y_lisbon_test, y_pred_test_lisbon))\n",
    "\n",
    "MSE_porto = mean_squared_error(y_porto_test, y_pred_test_porto)\n",
    "MSE_lisbon = mean_squared_error(y_lisbon_test, y_pred_test_lisbon)\n",
    "\n",
    "r2_porto = r2_score(y_porto_test, y_pred_test_porto)\n",
    "r2_lisbon = r2_score(y_lisbon_test, y_pred_test_lisbon)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"RMSE_porto: {round(RMSE_porto, 4)}\")\n",
    "print(f\"RMSE_lisbon: {round(RMSE_lisbon, 4)}\")\n",
    "\n",
    "print(f\"MSE_porto: {round(MSE_porto, 4)}\")\n",
    "print(f\"MSE_lisbon: {round(MSE_lisbon, 4)}\")\n",
    "\n",
    "print(f\"r2_porto: {round(r2_porto, 4)}\")\n",
    "print(f\"r2_lisbon: {round(r2_lisbon, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_porto_SS = porto_SS.drop('price', 1)\n",
    "y_porto_SS = porto_SS['price']\n",
    "\n",
    "X_lisbon_SS = lisbon_SS.drop('price', 1)\n",
    "y_lisbon_SS = lisbon_SS['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data\n",
    "X_porto_train, X_porto_test, y_porto_train, y_porto_test = train_test_split(X_porto_SS, y_porto_SS, test_size=0.2)\n",
    "\n",
    "X_lisbon_train, X_lisbon_test, y_lisbon_train, y_lisbon_test = train_test_split(X_lisbon_SS, y_lisbon_SS, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = xgb.XGBRegressor()\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300],\n",
    "              'learning_rate': [0.01, 0.05, 0.1], \n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.6, 0.7, 1],\n",
    "              'gamma': [0.0, 0.1, 0.2]}\n",
    "\n",
    "booster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tuned random forest porto\n",
    "booster_grid_search.fit(X_porto_train, y_porto_train)\n",
    "\n",
    "print(booster_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tuned random forest lisbon\n",
    "booster_grid_search.fit(X_lisbon_train, y_lisbon_train)\n",
    "\n",
    "print(booster_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the models based on the hyperparameters found by grid search\n",
    "booster_porto = xgb.XGBRegressor(colsample_bytree = 1, gamma = 0.0, learning_rate = 0.1, \n",
    "                           max_depth= 3, n_estimators = 300, random_state = 4)\n",
    "\n",
    "booster_lisbon = xgb.XGBRegressor(colsample_bytree = 1, gamma = 0.0, learning_rate = 0.05, \n",
    "                           max_depth = 4, n_estimators = 300, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "booster_porto.fit(X_porto_train, y_porto_train)\n",
    "\n",
    "booster_lisbon.fit(X_lisbon_train, y_lisbon_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred_train_porto = booster_porto.predict(X_porto_train)\n",
    "y_pred_test_porto = booster_porto.predict(X_porto_test)\n",
    "\n",
    "y_pred_train_lisbon = booster_lisbon.predict(X_lisbon_train)\n",
    "y_pred_test_lisbon = booster_lisbon.predict(X_lisbon_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics to determine quality of model\n",
    "RMSE_porto = np.sqrt(mean_squared_error(y_porto_test, y_pred_test_porto))\n",
    "RMSE_lisbon = np.sqrt(mean_squared_error(y_lisbon_test, y_pred_test_lisbon))\n",
    "\n",
    "MSE_porto = mean_squared_error(y_porto_test, y_pred_test_porto)\n",
    "MSE_lisbon = mean_squared_error(y_lisbon_test, y_pred_test_lisbon)\n",
    "\n",
    "r2_porto = r2_score(y_porto_test, y_pred_test_porto)\n",
    "r2_lisbon = r2_score(y_lisbon_test, y_pred_test_lisbon)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"RMSE_porto: {round(RMSE_porto, 4)}\")\n",
    "print(f\"RMSE_lisbon: {round(RMSE_lisbon, 4)}\")\n",
    "\n",
    "print(f\"MSE_porto: {round(MSE_porto, 4)}\")\n",
    "print(f\"MSE_lisbon: {round(MSE_lisbon, 4)}\")\n",
    "\n",
    "print(f\"r2_porto: {round(r2_porto, 4)}\")\n",
    "print(f\"r2_lisbon: {round(r2_lisbon, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the Feature selection notebook I came up with a list of features that could be good candidates for reduced data approach to modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisbon_list = ['last_review_present',\n",
    " 'neighbourhood_cleansed_Aldeia Galega da Merceana e Aldeia Gavinha',\n",
    " 'accommodates',\n",
    " 'host_name_present',\n",
    " 'price_per_guest',\n",
    " 'neighbourhood_cleansed_Arruda dos Vinhos',\n",
    " 'neighbourhood_cleansed_Cardosas',\n",
    " 'neighbourhood_cleansed_Arranh',\n",
    " 'neighbourhood_group_cleansed_Arruda Dos Vinhos',\n",
    " 'neighbourhood_cleansed_Mina de gua',\n",
    " 'neighbourhood_cleansed_Falagueira-Venda Nova',\n",
    " 'neighbourhood_cleansed_Alfragide',\n",
    " 'neighbourhood_cleansed_Venteira',\n",
    " 'neighbourhood_group_cleansed_Amadora',\n",
    " 'neighbourhood_cleansed_guas Livres',\n",
    " 'neighbourhood_cleansed_Sapataria',\n",
    " 'neighbourhood_cleansed_Santo Quintino',\n",
    " 'is_business_travel_ready',\n",
    " 'neighbourhood_cleansed_Sobral de Monte Agrao',\n",
    " 'neighbourhood_group_cleansed_Sobral De Monte Agrao',\n",
    " 'neighbourhood_cleansed_Encosta do Sol',\n",
    " 'reviews_p_month_present',\n",
    " 'neighbourhood_cleansed_Alcoentre',\n",
    " 'neighbourhood_cleansed_Pvoa de Santa Iria e Forte da Casa',\n",
    " 'neighbourhood_cleansed_Alhandra, So Joo dos Montes e Calhandriz',\n",
    " 'neighbourhood_group_cleansed_Azambuja',\n",
    " 'neighbourhood_group_cleansed_Vila Franca De Xira',\n",
    " 'neighbourhood_cleansed_Castanheira do Ribatejo e Cachoeiras',\n",
    " 'neighbourhood_cleansed_Manique do Intendente, V.N.de S.Pedro e Maussa',\n",
    " 'neighbourhood_cleansed_Vialonga',\n",
    " 'neighbourhood_cleansed_Vila Franca de Xira',\n",
    " 'neighbourhood_cleansed_Azambuja',\n",
    " 'neighbourhood_cleansed_Vale do Paraso',\n",
    " 'neighbourhood_cleansed_Alverca do Ribatejo e Sobralinho']\n",
    "\n",
    "porto_list = ['reviews_p_month_present',\n",
    " 'host_since_present',\n",
    " 'property_type_Windmill',\n",
    " 'last_review_present',\n",
    " 'neighbourhood_cleansed_São João de Ver',\n",
    " 'neighbourhood_cleansed_Campo e Sobrado',\n",
    " 'price_per_guest',\n",
    " 'accommodates',\n",
    " 'first_review_present',\n",
    " 'has_availability',\n",
    " 'neighbourhood_cleansed_Roge',\n",
    " 'neighbourhood_cleansed_São Pedro de Castelões',\n",
    " 'neighbourhood_cleansed_Cepelos',\n",
    " 'neighbourhood_cleansed_Caldas de São Jorge e de Pigeiros',\n",
    " 'neighbourhood_cleansed_Fiães',\n",
    " 'neighbourhood_cleansed_Romariz',\n",
    " 'neighbourhood_group_cleansed_SANTA MARIA DA FEIRA',\n",
    " 'neighbourhood_cleansed_Arões',\n",
    " 'neighbourhood_cleansed_Macieira de Cambra',\n",
    " 'neighbourhood_group_cleansed_VALE DE CAMBRA',\n",
    " 'neighbourhood_group_cleansed_SÃO JOÃO DA MADEIRA',\n",
    " 'neighbourhood_cleansed_Sanguedo',\n",
    " 'neighbourhood_cleansed_Santa Maria da Feira, Travanca, Sanfins e Espargo',\n",
    " 'neighbourhood_cleansed_Lourosa',\n",
    " 'neighbourhood_cleansed_Lobão, Gião, Louredo e Guisande',\n",
    " 'neighbourhood_cleansed_São João da Madeira',\n",
    " 'neighbourhood_cleansed_Canedo, Vale e Vila Maior',\n",
    " 'neighbourhood_cleansed_Argoncilhe',\n",
    " 'neighbourhood_cleansed_Bougado (São Martinho e Santiago)',\n",
    " 'neighbourhood_cleansed_Coronado (São Romão e São Mamede)',\n",
    " 'neighbourhood_cleansed_Mozelos',\n",
    " 'neighbourhood_cleansed_Muro',\n",
    " 'is_business_travel_ready',\n",
    " 'neighbourhood_cleansed_Alvarelhos e Guidões',\n",
    " 'neighbourhood_cleansed_Covelas',\n",
    " 'neighbourhood_group_cleansed_TROFA',\n",
    " 'neighbourhood_cleansed_Ermesinde',\n",
    " 'neighbourhood_cleansed_Paredes',\n",
    " 'neighbourhood_cleansed_Gandra',\n",
    " 'neighbourhood_cleansed_Alfena',\n",
    " 'neighbourhood_cleansed_Rebordosa',\n",
    " 'neighbourhood_cleansed_Cete',\n",
    " 'neighbourhood_cleansed_Aguiar de Sousa',\n",
    " 'neighbourhood_group_cleansed_VALONGO',\n",
    " 'neighbourhood_group_cleansed_PAREDES',\n",
    " 'neighbourhood_cleansed_Baltar',\n",
    " 'host_name_present',\n",
    " 'neighbourhood_cleansed_Valongo',\n",
    " 'neighbourhood_cleansed_Sobreira',\n",
    " 'neighbourhood_cleansed_Parada de Todeia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_porto_f = porto[porto_list]\n",
    "y_porto_f = porto.price.values\n",
    "\n",
    "X_lisbon_f = lisbon[lisbon_list]\n",
    "y_lisbon_f = lisbon.price.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets run the models with the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data\n",
    "X_porto_train, X_porto_test, y_porto_train, y_porto_test = train_test_split(X_porto_f, y_porto_f, test_size=0.33)\n",
    "\n",
    "X_lisbon_train, X_lisbon_test, y_lisbon_train, y_lisbon_test = train_test_split(X_lisbon_f, y_lisbon_f, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no scaling and no dimensionality reduction\n",
    "no_scale_no_pca = Pipeline([('linear', linear_model.LinearRegression())])\n",
    "no_scale_no_pca.fit(X_porto_train, y_porto_train)\n",
    "y_pred1_porto = no_scale_no_pca.predict(X_porto_test)\n",
    "linear_reg_err1_porto = metrics.median_absolute_error(y_porto_test, y_pred1_porto)\n",
    "linear_reg_r21_porto = metrics.r2_score(y_porto_test, y_pred1_porto)\n",
    "\n",
    "\n",
    "#with scaling, no dimensionality reduction\n",
    "scale_no_pca = Pipeline([('standardize', StandardScaler()), ('linear', linear_model.LinearRegression())])\n",
    "scale_no_pca.fit(X_porto_train, y_porto_train)\n",
    "y_pred2_porto = scale_no_pca.predict(X_porto_test)\n",
    "linear_reg_err2_porto = metrics.median_absolute_error(y_porto_test, y_pred2_porto)\n",
    "linear_reg_r22_porto = metrics.r2_score(y_porto_test, y_pred2_porto)\n",
    "\n",
    "    \n",
    "#scaling and dimensionality reduction\n",
    "scale_pca = Pipeline([('standardize', StandardScaler()),\n",
    "    ('pca', PCA(n_components = 20)), ('linear', linear_model.LinearRegression())])\n",
    "scale_pca.fit(X_porto_train, y_porto_train)\n",
    "y_pred3_porto = scale_pca.predict(X_porto_test)\n",
    "linear_reg_err3_porto = metrics.median_absolute_error(y_porto_test, y_pred3_porto)\n",
    "linear_reg_r23_porto = metrics.r2_score(y_porto_test, y_pred3_porto)\n",
    "\n",
    "\n",
    "print (\"Linear Regression's MAE without PCA or scaling in Porto: \" + str(linear_reg_err1_porto))\n",
    "print (\"Linear Regression's MAE deviation with scaling in Porto: \" + str(linear_reg_err2_porto))\n",
    "print (\"Linear Regression's MAE deviation with PCA and scaling in Porto: \" + str(linear_reg_err3_porto))\n",
    "\n",
    "print (\"Linear Regression's r2 without PCA or scaling in Porto: \" + str(linear_reg_r21_porto))\n",
    "print (\"Linear Regression's r2 deviation with scaling in Porto: \" + str(linear_reg_r22_porto))\n",
    "print (\"Linear Regression's r2 deviation with PCA and scaling in Porto: \" + str(linear_reg_r23_porto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no scaling and no dimensionality regression\n",
    "no_scale_no_pca = Pipeline([('linear', linear_model.LinearRegression())])\n",
    "no_scale_no_pca.fit(X_lisbon_train, y_lisbon_train)\n",
    "y_pred1_lisbon = no_scale_no_pca.predict(X_lisbon_test)\n",
    "linear_reg_err1_lisbon = metrics.median_absolute_error(y_lisbon_test, y_pred1_lisbon)\n",
    "linear_reg_r21_lisbon = metrics.r2_score(y_lisbon_test, y_pred1_lisbon)\n",
    "\n",
    "\n",
    "#with scaling, no dimensionality regression\n",
    "scale_no_pca = Pipeline([('standardize', StandardScaler()), ('linear', linear_model.LinearRegression())])\n",
    "scale_no_pca.fit(X_lisbon_train, y_lisbon_train)\n",
    "y_pred2_lisbon = scale_no_pca.predict(X_lisbon_test)\n",
    "linear_reg_err2_lisbon = metrics.median_absolute_error(y_lisbon_test, y_pred2_lisbon)\n",
    "linear_reg_r22_lisbon = metrics.r2_score(y_lisbon_test, y_pred2_lisbon)\n",
    "\n",
    "    \n",
    "#scaling and dimensionality reduction\n",
    "scale_pca = Pipeline([('standardize', StandardScaler()),\n",
    "    ('pca', PCA(n_components = 20)), ('linear', linear_model.LinearRegression())])\n",
    "scale_pca.fit(X_lisbon_train, y_lisbon_train)\n",
    "y_pred3_lisbon = scale_pca.predict(X_lisbon_test)\n",
    "linear_reg_err3_lisbon = metrics.median_absolute_error(y_lisbon_test, y_pred3_lisbon)\n",
    "linear_reg_r23_lisbon = metrics.r2_score(y_lisbon_test, y_pred3_lisbon)\n",
    "\n",
    "\n",
    "print (\"Linear Regression's MAE without PCA or scaling in Lisbon: \" + str(linear_reg_err1_lisbon))\n",
    "print (\"Linear Regression's MAE deviation with scaling in Lisbon: \" + str(linear_reg_err2_lisbon))\n",
    "print (\"Linear Regression's MAE deviation with PCA and scaling in Lisbon: \" + str(linear_reg_err3_lisbon))\n",
    "print (\"Linear Regression's r2 without PCA or scaling in Lisbon: \" + str(linear_reg_r21_lisbon))\n",
    "print (\"Linear Regression's r2 deviation with scaling in Lisbon: \" + str(linear_reg_r22_lisbon))\n",
    "print (\"Linear Regression's r2 deviation with PCA and scaling in Lisbon: \" + str(linear_reg_r23_lisbon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with conservative approach to dropped and cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
