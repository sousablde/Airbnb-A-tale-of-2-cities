{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sousa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sousa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sousa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from math import sqrt\n",
    "import tests as t\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import folium\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import xgboost as xgb\n",
    "pd.set_option('display.max_columns', 106)\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto = pd.read_csv(r\"C:\\Users\\sousa\\Desktop\\github\\Airbnb Tale of 2 cities\\Data Portugal\\porto_listings.csv\")\n",
    "lisbon = pd.read_csv(r\"C:\\Users\\sousa\\Desktop\\github\\Airbnb Tale of 2 cities\\Data Portugal\\lisbon_listings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_porto = porto.drop('price', 1)\n",
    "y_porto = porto['price']\n",
    "\n",
    "X_lisbon = lisbon.drop('price', 1)\n",
    "y_lisbon = lisbon['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data\n",
    "X_porto_train, X_porto_test, y_porto_train, y_porto_test = train_test_split(X_porto, y_porto, test_size=0.2)\n",
    "\n",
    "X_lisbon_train, X_lisbon_test, y_lisbon_train, y_lisbon_test = train_test_split(X_lisbon, y_lisbon, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "sc = StandardScaler()\n",
    "X_porto_train = sc.fit_transform(X_porto_train)\n",
    "X_porto_test  = sc.transform(X_porto_test)\n",
    "\n",
    "X_lisbon_train = sc.fit_transform(X_lisbon_train)\n",
    "X_lisbon_test  = sc.transform(X_lisbon_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = xgb.XGBRegressor()\n",
    "\n",
    "param_grid = {'n_estimators': [100, 150, 200],\n",
    "              'learning_rate': [0.01, 0.05, 0.1], \n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.6, 0.7, 1],\n",
    "              'gamma': [0.0, 0.1, 0.2]}\n",
    "\n",
    "booster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tuned random forest porto\n",
    "booster_grid_search.fit(X_porto_train, y_porto_train)\n",
    "\n",
    "print(booster_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tuned random forest lisbon\n",
    "booster_grid_search.fit(X_lisbon_train, y_lisbon_train)\n",
    "\n",
    "print(booster_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
